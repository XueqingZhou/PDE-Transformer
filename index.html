<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, customized for PDE-Transformer -->
  <meta name="description" content="Project page for &quot;PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling&quot;, arXiv 2025.">
  <meta property="og:title" content="PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling"/>
  <meta property="og:description" content="Project page for arXiv 2025 paper on PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling."/>
  <meta property="og:url" content="https://github.com/XueqingZhou/PDE-Transformer"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="assets/images/teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling">
  <meta name="twitter:description" content="arXiv 2025 project page for PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="assets/images/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="PDE-Transformer, continuous dynamical systems, sequence modeling, Transformers, long-range dependencies, arXiv 2025, deep learning, machine learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PDE-Transformer</title>
  <link rel="icon" type="image/x-icon" href="assets/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="assets/css/bulma.min.css">
  <link rel="stylesheet" href="assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="assets/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="assets/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="assets/js/fontawesome.all.min.js"></script>
  <script src="assets/js/bulma-carousel.min.js"></script>
  <script src="assets/js/bulma-slider.min.js"></script>
  <script src="assets/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <span>Yukun Zhang</span><sup>*</sup>,</span>
              <span class="author-block">
                <span>Xueqing Zhou</span><sup>*</sup>
              </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The Chinese University of Hong Kong &nbsp;&nbsp;&nbsp; Fudan University<br><b>arXiv 2025</b></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- ArXiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2510.03272" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span>

                      <!-- Github link -->
                      <span class="link-block">
                        <a href="https://github.com/XueqingZhou/PDE-Transformer" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                        </a>
                      </span>

                      <!-- ArXiv Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2510.03272" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                        </a>
                      </span>

                  </div>
              </div>
            </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/images/teaser.png" alt="Teaser"/>
      <h2 class="subtitle has-text-centered">
        PDE-Transformer casts the forward pass of a Transformer as the numerical discretization of a continuous reaction-diffusion system,
        enabling efficient and principled modeling of long-range dependencies by harmonizing continuous PDE smoothing with discrete self-attention.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <b>TL;DR:</b> We propose PDE-Transformer, a novel sequence modeling paradigm that casts the forward pass of a Transformer as the numerical discretization 
    of a continuous reaction-diffusion system derived from a variational energy functional, offering a principled, lightweight mechanism to bolster 
    long-range dependency modeling.
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose <b>PDE-Transformer</b>, a novel sequence modeling paradigm that casts the forward pass of a Transformer as the numerical 
            discretization of a continuous reaction-diffusion system derived from a variational energy functional. In our framework, token embeddings 
            evolve under a partial differential equation whose nonlocal integral term models self-attention, local reaction term models feed-forward 
            layers, diffusion term encodes positional smoothing, and a stability control term corresponds to layer normalization. From this unifying 
            perspective, we design an <b>Adaptive PDE Diffusion Layer</b>â€”an efficient, learnable finite-difference stencil that enforces local smoothness 
            in feature space with linear time complexity and complements self-attention's global routing. Through a systematic theoretical analysis based 
            on four pillars: stability, diffusion geometry, multi-scale dynamics, and component coupling, we derive principled guidelines for integrating 
            the PDE layer at seven candidate points in the Transformer. Empirically, on the Long Range Arena benchmark, placing the layer immediately after 
            embedding yields a 4.1 pp average accuracy gain over a strong baseline, and an adaptive multi-scale variant delivers further improvements. 
            Our work thus offers a principled, lightweight mechanism to bolster long-range dependency modeling by harmonizing continuous PDE smoothing 
            with discrete self-attention.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
        </div>
      </div>
    </div>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="assets/images/homonym_explain.png" alt="Homonyms Explanation"/>
        <h2 class="subtitle has-text-centered">
          Homonyms. We see that the word "baseball" provides the required synergistic context with the homonym "bat" to pick the 
          sports setting over the animal. This effect can be confirmed in the synergy maps and image-level synergy values (S) as 
          well where we observe a high synergy for "bat" with "baseball" compared to other words such as "He" and "swung".
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="assets/images/homonym_result.png" alt="Homonyms Result"/>
        <h2 class="subtitle has-text-centered">
          Homonyms. Left: Successful generation of homonym "bowl" in different contexts due to high synergy with modifiers "bowl" 
          and "game". Right: Failure case where the model generates the homonym "mole" with the same semantic meaning, the animal, 
          due to its failure to use contextual information from words like "coworker" as can be seen in the synergy map.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="assets/images/synonym_result.png" alt="Synonyms Results"/>
        <h2 class="subtitle has-text-centered">
          Synonyms. Our redundancy map is able to highlight that the model considers the pairs "bed" and "mattress" (left) and "cube" 
          and "cuboid" (right) as semantically similar
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="assets/images/hypo_result_coco.png" alt="Co-Hyponyms Results"/>
      <h2 class="subtitle has-text-centered">
        COCO Co-Hyponyms. The redundancy map proves to be very useful in finding out the reason behind the model's failures in these figures. 
        It confuses the co-hyponym pairs ("sandwich", "pizza")(left) and ("elephant", "cat")(right) to have the same meaning for the co-hyponyms 
        as seen from the redundancy maps, which results in erroneous generations
      </h2>
    </div>
     <div class="item">
      <!-- Your image here -->
      <img src="assets/images/p_intervention_r1.png" alt="Prompt Intervention Results"/>
      <h2 class="subtitle has-text-centered">
        Prompt Intervention. The redundancy is highly activated in the face region of the giraffe. On a closer look, we see that the face is that 
        of a cow meaning that the word "cow" is redundant. This is confirmed as the image changes very little on omitting it from the prompt.
      </h2>
    </div>
     <div class="item">
      <!-- Your image here -->
      <img src="assets/images/uniqueness_mrf_new.png" alt="Uniqueness Results"/>
      <h2 class="subtitle has-text-centered">
        Most Representative Features. Left: The "toothbrush" uniqueness map correctly captures the toothbrush bristles, their most distinct feature. 
        Right: The "bear" uniqueness map correctly captures the bear region, specifically the face.
      </h2>
    </div>
  </div>
    </div>
  </div>
</section>
<!-- End image carousel -->





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{zhang2025pde,
  title={PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling},
  author={Zhang, Yukun and Zhou, Xueqing},
  journal={arXiv preprint arXiv:2510.03272},
  year={2025},
  url={https://arxiv.org/abs/2510.03272}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This template was borrowed from <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> 
            project page under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
