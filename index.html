<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, customized for PDE-Transformer -->
  <meta name="description" content="Project page for &quot;PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling&quot;, arXiv 2025.">
  <meta property="og:title" content="PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling"/>
  <meta property="og:description" content="Project page for arXiv 2025 paper on PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling."/>
  <meta property="og:url" content="https://github.com/XueqingZhou/PDE-Transformer"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="assets/images/teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling">
  <meta name="twitter:description" content="arXiv 2025 project page for PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="assets/images/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="PDE-Transformer, continuous dynamical systems, sequence modeling, Transformers, long-range dependencies, arXiv 2025, deep learning, machine learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PDE-Transformer</title>
  <link rel="icon" type="image/x-icon" href="assets/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="assets/css/bulma.min.css">
  <link rel="stylesheet" href="assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="assets/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="assets/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="assets/js/fontawesome.all.min.js"></script>
  <script src="assets/js/bulma-carousel.min.js"></script>
  <script src="assets/js/bulma-slider.min.js"></script>
  <script src="assets/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <span>Yukun Zhang</span><sup>*</sup>,</span>
              <span class="author-block">
                <span>Xueqing Zhou</span><sup>*</sup>
              </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The Chinese University of Hong Kong &nbsp;&nbsp;&nbsp; Fudan University<br><b>arXiv 2025</b></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- ArXiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2510.03272" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span>

                      <!-- Github link -->
                      <span class="link-block">
                        <a href="https://github.com/XueqingZhou/PDE-Transformer" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                        </a>
                      </span>

                      <!-- ArXiv Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2510.03272" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                        </a>
                      </span>

                  </div>
              </div>
            </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/images/teaser.png" alt="Teaser"/>
      <h2 class="subtitle has-text-centered">
        PDE-Transformer casts the forward pass of a Transformer as the numerical discretization of a continuous reaction-diffusion system,
        enabling efficient and principled modeling of long-range dependencies by harmonizing continuous PDE smoothing with discrete self-attention.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <b>TL;DR:</b> We propose PDE-Transformer, a novel sequence modeling paradigm that casts the forward pass of a Transformer as the numerical discretization 
    of a continuous reaction-diffusion system derived from a variational energy functional, offering a principled, lightweight mechanism to bolster 
    long-range dependency modeling.
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose <b>PDE-Transformer</b>, a novel sequence modeling paradigm that casts the forward pass of a Transformer as the numerical 
            discretization of a continuous reaction-diffusion system derived from a variational energy functional. In our framework, token embeddings 
            evolve under a partial differential equation whose nonlocal integral term models self-attention, local reaction term models feed-forward 
            layers, diffusion term encodes positional smoothing, and a stability control term corresponds to layer normalization. From this unifying 
            perspective, we design an <b>Adaptive PDE Diffusion Layer</b>â€”an efficient, learnable finite-difference stencil that enforces local smoothness 
            in feature space with linear time complexity and complements self-attention's global routing. Through a systematic theoretical analysis based 
            on four pillars: stability, diffusion geometry, multi-scale dynamics, and component coupling, we derive principled guidelines for integrating 
            the PDE layer at seven candidate points in the Transformer. Empirically, on the Long Range Arena benchmark, placing the layer immediately after 
            embedding yields a 4.1 pp average accuracy gain over a strong baseline, and an adaptive multi-scale variant delivers further improvements. 
            Our work thus offers a principled, lightweight mechanism to bolster long-range dependency modeling by harmonizing continuous PDE smoothing 
            with discrete self-attention.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
        </div>
      </div>
    </div>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <!-- Add result images here -->
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{zhang2025pde,
  title={PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling},
  author={Zhang, Yukun and Zhou, Xueqing},
  journal={arXiv preprint arXiv:2510.03272},
  year={2025},
  url={https://arxiv.org/abs/2510.03272}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This template was borrowed from <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> 
            project page under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
